{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"The power of Word2Vec\"\n",
    "date: 2020-06-27\n",
    "updated: 2020-06-27\n",
    "author: \"Pratyush Ravishankar\"\n",
    "contact: \"https://www.linkedin.com/in/pratyush-ravishankar-a5391615a/\"\n",
    "tags:\n",
    "- NLP\n",
    "- word-embeddings\n",
    "categories:\n",
    "- [NLP]\n",
    "- [Neural Net]\n",
    "languages:\n",
    "- Python\n",
    "description: \"This post provides interesting examples of \"\n",
    "cover: /banners/gibrats-law.jpg\n",
    "katex: true\n",
    "---"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "{% note info %}\n",
    "**Accessing Post Source**\n",
    "We are still working on getting this site set up, so source code for this post is not yet available. Check back soon and you'll be able to find it linked here.\n",
    "{% endnote %}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'helpers'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-e99608466f76>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscripts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglove2word2vec\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mglove2word2vec\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mhelpers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDownloadProgressBar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpprint\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mglove_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabspath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'resources/glove.bin'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'helpers'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "\n",
    "from helpers import DownloadProgressBar, pprint\n",
    "\n",
    "glove_path = os.path.abspath('resources/glove.bin')\n",
    "w2v_path = os.path.abspath('resources/w2v.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(w2v_path):\n",
    "    if not os.path.exists(glove_path):\n",
    "        raise OSError(f\"No file found at {glove_path}. Either change this \",\n",
    "                      \"path in the script configuration or download a pre-\",\n",
    "                      \"trained model from nlp.stanford.edu/projects/glove/\")\n",
    "    glove2word2vec(glove_path, w2v_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = KeyedVectors.load_word2vec_format(glove_path, binary=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why word embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Human language seem intuitive to humans, but not so much to a computer. We can determine whether two words share a similar meaning, but if asked to give a quantivative reason why that is the case, we will start to stuggle. But our shortcoming is where computers thrive, as they are remarkably good with numbers and tabular data.  Word embeddings look to provide words with this computer-friendly format, so we can reap the rewards of NLP."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The need for Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initial attempts before word embeddings included a one-hot-encoding. Essentially, each word is provided a vector which distinguishes it from other words in the vocabulary, as each vector is othogonal to every other vector.  This, isn't very practical to model anything on the scale of human languages, as the vectors dimensions would be the size of the language. But more importantly, this doesn't capture any sort of similarity between any two words. As far this method is concerned, the words \"dolphin\" and \"neoliberal\" are equally similar to \"shark\". Word2Vec aims to solve this problem by providing word-embeddings which take into account relations between words. In essence, Word2Vec provides a canvas ( albeit a strange multi-dimensional one) where any word in the language could lie, and effectively plots points where each word lies on this canvas. The closer any two points on this canvas lie (captured mathematically by the cosine distance), the more likely we humans are to describe the respresented words as \"similar\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'KeyedVectors' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-bf8ef8a272e6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mvis_glove_path\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabspath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'resources/glovevis.txt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mglove_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mKeyedVectors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_word2vec_format\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvis_glove_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbinary\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'KeyedVectors' is not defined"
     ]
    }
   ],
   "source": [
    "vis_glove_path= os.path.abspath('resources/glovevis.txt')\n",
    "glove_model = KeyedVectors.load_word2vec_format(vis_glove_path, binary=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_dimensions(model):\n",
    "    num_dimensions = 2  # final num dimensions (2D, 3D, etc)\n",
    "    print(\"reduce dimensions\")\n",
    "\n",
    "    vectors = [] # positions in vector space\n",
    "    labels = [] # keep track of words to label our data again later\n",
    "    for idx, word in enumerate(model.vocab):\n",
    "        print(idx)\n",
    "        vectors.append(model[word])\n",
    "        labels.append(word)\n",
    "\n",
    "    # convert both lists into numpy vectors for reduction\n",
    "    vectors = np.asarray(vectors)\n",
    "    labels = np.asarray(labels)\n",
    "    # reduce using t-SNE\n",
    "    vectors = np.asarray(vectors)\n",
    "    tsne = TSNE(n_components=num_dimensions, random_state=0)\n",
    "    vectors = tsne.fit_transform(vectors)\n",
    "    x_vals = [v[0] for v in vectors]\n",
    "    y_vals = [v[1] for v in vectors]\n",
    "    return x_vals, y_vals, labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_with_matplotlib(x_vals, y_vals, labels):\n",
    "    # import matplotlib.pyplot as plt\n",
    "    # import random\n",
    "\n",
    "    print(\"plot with matplotlib\")\n",
    "\n",
    "    random.seed(0)\n",
    "\n",
    "    plt.figure(figsize=(12, 12))\n",
    "    plt.scatter(x_vals, y_vals)\n",
    "\n",
    "    #\n",
    "    # Label randomly subsampled 25 data points\n",
    "    #\n",
    "    indices = list(range(len(labels)))\n",
    "    selected_indices = random.sample(indices, 25)\n",
    "    for i in selected_indices:\n",
    "        plt.annotate(labels[i], (x_vals[i], y_vals[i]))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_vals, y_vals, labels = reduce_dimensions(glove_model)\n",
    "plot_function = plot_with_matplotlib\n",
    "plot_function(x_vals, y_vals, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is commonality? - deriving the word embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To derive each word embedding, the Word2Vec model is usually trained using a method called Skipgram with Negative Sampling (SGNS). Essentially a large corpus (billions of words) is fed to the model. An n-sized sliding window is used to capture the words that lie either side of each word in the corpus, to determine each word's context. In simplified terms, the context for each word is used to determine the words embedding vector, in addition to added noise - random words which will most likely have nothing to do with the target word, hence the negative sampling aspect. Because words with a similar context usually have closely-linked meanings, these words will end up having similar embedding vectors.\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "![](/images/word2vec/sliding_window.png)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "{% note success %}\n",
    "On iteration j and k, \"fox\" and \"bear have similar contexts, so will end up with relatively close embedding vectors. After many adjustments to their vectors each time they are found in the corpus, their vectors will provide an increasingly accurate represention of the \"fox\" and \"bear\" relation - types of animals. \n",
    "{% endnote %}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The power of Word2Vec "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can find the commonality between words based on their cosine distance in the vector space. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.38950953"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.similarity('Queen', 'throne')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.002614806"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.similarity('Queen', 'forklift')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.20833212"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.similarity('Queen', 'Bowie')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, the \"forklift\" is relatively distanct from \"Queen\" compared to \"throne\". But what's fascinating is tht multiple facets of Queen are captured. \"Bowie\" is also relatively close to \"Queen\" like \"throne\", but instead certainly because of its relations to the iconic rock band."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naturally with vectors come mathematical operations and the real power of Word2Vec starts to shine. Vector differences are the crux behind **Analogies** which are best explained through examples...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analogies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "{% note info %}\n",
    "Analogies derived from the model trained on the [Google News dataset](https://code.google.com/archive/p/word2vec/)\n",
    "{% endnote %}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A classic: king - man + woman -> ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "queen (0.712)\n"
     ]
    }
   ],
   "source": [
    "pprint(model.most_similar(positive=['king', 'woman'], negative=['man'], topn=1))\n",
    "# king - man + woman"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This example is rather intuitive; the female version of the male title \"king\" is \"queen\". What's going is that vector sum of \"woman\" and the vector difference of \"king\" and \"man\" gives a vector which is rather similar to the vector \"queen\".  In other words, the (\"king\" - \"man\") vector is approximately equal to the (\"Queen\" - \"woman\") vector."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plurals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With a rather mundane example such as bikes - bike + gloves, it's not unsurprising the model returns glove; it could be got from deciphering that the pattern is removing the \"s\" - hardly groundbreaking. But when talking about irregular plurals, the required task to output the derived word shifts from spotting a simple pattern to seemingly needing a human-like understanding of the structure and complexities of the english language. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feet (0.568)\n"
     ]
    }
   ],
   "source": [
    "pprint(model.most_similar(positive=['cacti', 'foot'], negative=['cactus'], topn=1))\n",
    "# cacti - cactus + foot -> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "children (0.726)\r\n",
      "infant (0.702)\r\n",
      "chid (0.685)\n"
     ]
    }
   ],
   "source": [
    "pprint(model.most_similar(positive=['sheep', 'child'], negative=['sheep'], topn=3))\n",
    "# sheep - sheep -> child"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, \"sheep\" is both the singular and the plural, meaning the resulting vector is simply just \"child\". But \"children\" is still found due to its similarity with \"child\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Geographical analogies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find the city..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lisbon (0.655)\n"
     ]
    }
   ],
   "source": [
    "pprint(model.most_similar(positive=['Moscow', 'Portugal'], negative=['Russia'], topn=1))\n",
    "# Moscow - Russia + Portugal -> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and now find the country..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "India (0.626)\n"
     ]
    }
   ],
   "source": [
    "pprint(model.most_similar(positive=['Spain', 'Delhi'], negative=['Barcelona'], topn=1))\n",
    "# Spain - Barcelona + Delhi ->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pprint' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-51f153db0107>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmost_similar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpositive\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Africa'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Cambodia'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnegative\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Egypt'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtopn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m# Africa - Egypt + Cambodia ->\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pprint' is not defined"
     ]
    }
   ],
   "source": [
    "pprint(model.most_similar(positive=['Africa', 'Cambodia'], negative=['Egypt'], topn=1))\n",
    "# Africa - Egypt + Cambodia ->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pprint' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-f817d50c5b7e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmost_similar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpositive\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Iran'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'war'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtopn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m# Iran + war ->\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pprint' is not defined"
     ]
    }
   ],
   "source": [
    "pprint(model.most_similar(positive=['Iran', 'war'],topn=3))\n",
    "# Iran + war ->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An interesting example. \"Iraq\" and \"Islamic_republic\" are most likely referencing the Iran-Iraq war. In addition \"Iraq\" and \"Syria\", are both war-stricken countries near Iran which could be a factor why they're near the (Iran + war) vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Opposites\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With a well-known opposite, the result makes sense..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(model.most_similar(positive=['big', 'high'], negative=['small'], topn=1))\n",
    "# big - small + high ->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...but a word with no obvious opposite can give rather surprising results. Ever wondered what the opposite of a \"bottle\" is? Look no further..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Publish_Date (0.266)\r\n",
      "By_CHRIS_MARCHESO (0.252)\r\n",
      "BY_LYNN_ARDITI (0.252)\n"
     ]
    }
   ],
   "source": [
    "pprint(model.most_similar(negative=['bottle'], topn=1))\n",
    "#  -bottle ->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It'd make sense to view the vector representing the opposite of a word as the vector in the opposite direction. But usually this makes little sense as portrayed by this example, as it turns out human have very little understanding of what the \"opposite\" generally is.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A few vector sums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pretty printing has been turned OFF\n"
     ]
    }
   ],
   "source": [
    "pprint(model.most_similar(positive=['death', 'water'], topn=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pretty printing has been turned ON\n"
     ]
    }
   ],
   "source": [
    "pprint(model.most_similar(positive=['death', 'knife'], topn=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ex_girlfriend (0.517)\n"
     ]
    }
   ],
   "source": [
    "pprint(model.most_similar(positive=['girlfriend'], negative=['love'], topn=1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loved (0.580)\r\n",
      "friend (0.551)\n"
     ]
    }
   ],
   "source": [
    "pprint(model.most_similar(positive=['colleage', 'love'], topn=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Miscellaneous "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Medvedev (0.674)\r\n",
      "Putin (0.647)\r\n",
      "Kremlin (0.617)\n"
     ]
    }
   ],
   "source": [
    "pprint(model.most_similar(positive=['Obama', 'Russia'], negative=['USA'], topn=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tony_Blair (0.522)\r\n",
      "Oliver_Cromwell (0.509)\r\n",
      "Maggie_Thatcher (0.506)\n"
     ]
    }
   ],
   "source": [
    "pprint(model.most_similar(positive=['Hitler', 'UK'], negative=['Germany'], topn=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Make of that what you will..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steve_Jobs (0.523)\n"
     ]
    }
   ],
   "source": [
    "pprint(model.most_similar(positive=['Gates', 'Apple'], negative=['Microsoft'], topn=1))\n",
    "# Gates - Microsoft + Apple ->                          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(model.most_similar(positive=['Barack_Obama', 'Victoria_Beckham'], negative=['Michelle'], topn=1))\n",
    "# Barack_Obama - Michelle + Victoria_Beckham ->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(model.most_similar(positive=['Anfield', 'Manchester'], negative=['Liverpool'], topn=1))\n",
    "# Anfield - Liverpool + Manchester ->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "## Wider Applications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many application scenarios for Word2Vec: sentiment analysis, recommender systems, etc. But aside from these e-commerce centric use cases, it has also found usage in scientific fields such as BioNLP, which have utilised word embeddings for technological advancements. Hopefully through these examples, the potential power of Word2Vec has been showcased."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}